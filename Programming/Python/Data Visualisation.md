- process:
  - gather data into a rough database
  - clean the data up by processing it and inputting it into a second database
  - analyse or visualise the processed data

Web Crawler:
  - this is a program that runs to retrieve a web page
  - it looks through the web page looking for any links
  - it adds any links it finds into a list of "to retrieve links"

  - there is a "robots.txt" file on most web pages
  - this is designed to talk to the web crawlers and it is an informal voluntary guideline for the crawlers to follow

Page Rank:
  - this is where you take the links that you have crawled and you rank them with a number according to how useful they are with the links that they provide
  - index building - the start of the page ranking, building a base
  - search index - builds up which sites are the most used/ have the best value

Data Pipeline:
  - this is the follow the data will follow from the original source, to the crawler retrieving it, to the raw database, to processing the data and inputting it into the organised database, to visualising the organised data
